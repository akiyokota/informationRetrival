package CS172_Info_Ret.webCrawler;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.io.InputStreamReader;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.LinkedList;
import java.util.List;
import java.util.StringTokenizer;

import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;

import CS172_Info_Ret.webCrawler.Objects.NormalizedUrl;
import CS172_Info_Ret.webCrawler.Objects.robot;

public class Crawler {
	
	private List <NormalizedUrl> normalizedUrlList;
	private List <robot> robots;
	private Document doc;
	private String url;
	
	
	/**
	 * Constructor
	 */
	public Crawler (String url) {
		try {
			this.normalizedUrlList = new LinkedList<NormalizedUrl> ();
			this.robots = new LinkedList<robot> ();

			this.url = url;
			this.doc = Jsoup.connect(url).get();
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	/**
	 * Parse out all links from the webpage, and put these links into NormalizedUrl object
	 */
	public void linkExtraction () {
		try {
			List<URL> urlList = new LinkedList<URL> ();
			urlList = new ParseHTML().parseHTML(doc);
			
			for (URL u : urlList) {
				NormalizedUrl nUrl = new NormalizedUrl();
				nUrl.UrlNormalization(u);
				this.normalizedUrlList.add(nUrl);
			}

		} catch (Exception e) {
			e.printStackTrace();
		}
	}
	
	/**
	 * Download a webcontent the primative way
	 * @param path : the path of the folder
	 * @param fileName : name of the file that saves all the web content
	 * @param url : the url you wish to download
	 */
	public  void downloadPage(String path, String fileName, String url) throws IOException, MalformedURLException {
		URL urlObj = new URL(url);

		BufferedReader x = new BufferedReader (new InputStreamReader(urlObj.openConnection().getInputStream()));
		
		BufferedWriter fos = new BufferedWriter(new FileWriter( path + fileName));
		
		while(x.ready()) {
			String line = x.readLine();
			fos.write(line);
			fos.write("\n");
		}
		
		x.close();
		fos.close();
	}

	/**
	 * Download a webcontent the primative way
	 * @param url : the url you wish to download
	 * @return content of the page
	 */
	private  String fetchPageToMemory(String url) throws IOException, MalformedURLException {
		URL urlObj = new URL(url);
		StringBuilder content = new StringBuilder();
		BufferedReader x = new BufferedReader (new InputStreamReader(urlObj.openConnection().getInputStream()));
		
		
		while(x.ready()) {
			content.append(x.readLine());
			content.append("\n");
		}
		
		x.close();
		
		return content.toString();
	}
	
	private void printRobots() {
		for (robot r : this.robots) {
			System.out.println(r.getUserAgent());
			System.out.println(r.getCrawl_Delay());
			for(String s: r.getAllowList()) {
				System.out.println(s);
			}
			for(String s: r.getDisallowList()) {
				System.out.println(s);
			}
		}
	}
	
	public void ParseRobots() {
		try {
			String robots = fetchPageToMemory("http://www.about.com/robots.txt");
			StringTokenizer st = new StringTokenizer(robots, "\n");
			
			robot agent = new robot ();
			
			while(st.hasMoreElements()) {
				StringTokenizer split = new StringTokenizer(st.nextElement().toString(), ":");
				if(split.nextElement().toString().equals("User-agent")) {

					this.robots.add(agent);
					agent = new robot();

					agent.setUserAgent(split.nextElement().toString());
				}
			}
			
			printRobots();
			
		} catch (MalformedURLException e) {
			e.printStackTrace();
		} catch (IOException e) {
			e.printStackTrace();
		}
	}
	
	public List<NormalizedUrl> getNormalizedUrlList() {
		return normalizedUrlList;
	}



	public void setNormalizedUrlList(List<NormalizedUrl> normalizedUrlList) {
		this.normalizedUrlList = normalizedUrlList;
	}
}
